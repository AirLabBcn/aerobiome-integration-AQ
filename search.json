[
  {
    "objectID": "HVS-SASS.html",
    "href": "HVS-SASS.html",
    "title": "Integrating air microbiome for comprehensive air quality analysis",
    "section": "",
    "text": "Here, we conduct part of the analysis and generate figures for the manuscript Integrating the air microbiome for comprehensive air quality analysis.\nTo enhance readability, most of the code is hidden by default. You can expand it by clicking the “Show code” button above each cell. The complete code should be executable in a local environment, running smoothly from start to finish."
  },
  {
    "objectID": "HVS-SASS.html#data-loading-and-wrangling",
    "href": "HVS-SASS.html#data-loading-and-wrangling",
    "title": "Integrating air microbiome for comprehensive air quality analysis",
    "section": "Data loading and wrangling",
    "text": "Data loading and wrangling\nWe start by loading the medatata file which we have stored in /data/meta.txt with information about each sample:\n\nmetadata_df = pd.read_table('../data/meta.txt', sep='\\t')\nmetadata_df.head()\n\n\n\n\n\n\n\n\nSample Name\nSample Number\nAirLab_Code\nSeq ID\nSample ID\nSampling\nProject Name\nLocation\nRelative Location\nMedia Type\n...\nEntered By\nDNA Extraction Date (YYYY-MM-DD)\nDNA Extraction Kit\nDNA Concentration (ng/uL)\nDNA Volume of Elution (uL)\nDNA Quantification Instrument\nDNA Vol. Used for Quantification (uL)\nDNA Storage Freezer\nDNA Storage Drawer\nDNA Performed By\n\n\n\n\n0\n2022/11/23HVS-SASS-PilotA1\n1\nT6.0 B\nHVSP1\nC6-B\nContinuous\nHVS-SASS-Pilot\nAirLab Terrace\nOutdoor\nQuartz Filter\n...\nCassie Heinle\n28/11/2022\nPhe-Chlo method (AirLab Spain)\n7133\n17\nQuantus Promega (AirLab Spain)\n5\n-20C MPL Freezer (New)\nYee Hui\nAirLab Spain\n\n\n1\n2022/11/28HVS-SASS-PilotA2\n2\nT6.1 B\nHVSP2\nD6.1-B\nDiscrete\nHVS-SASS-Pilot\nAirLab Terrace\nOutdoor\nQuartz Filter\n...\nCassie Heinle\n28/11/2022\nPhe-Chlo method (AirLab Spain)\n856\n17\nQuantus Promega (AirLab Spain)\n5\n-20C MPL Freezer (New)\nYee Hui\nAirLab Spain\n\n\n2\n2022/11/28HVS-SASS-PilotA3\n3\nT6.2 B\nHVSP3\nD6.2-B\nDiscrete\nHVS-SASS-Pilot\nAirLab Terrace\nOutdoor\nQuartz Filter\n...\nCassie Heinle\n28/11/2022\nPhe-Chlo method (AirLab Spain)\n1193\n17\nQuantus Promega (AirLab Spain)\n5\n-20C MPL Freezer (New)\nYee Hui\nAirLab Spain\n\n\n3\n2022/11/28HVS-SASS-PilotA4\n4\nT6.3 B\nHVSP4\nD6.3-B\nDiscrete\nHVS-SASS-Pilot\nAirLab Terrace\nOutdoor\nQuartz Filter\n...\nCassie Heinle\n28/11/2022\nPhe-Chlo method (AirLab Spain)\n1483\n17\nQuantus Promega (AirLab Spain)\n5\n-20C MPL Freezer (New)\nYee Hui\nAirLab Spain\n\n\n4\n2022/11/28HVS-SASS-PilotA5\n5\nT6.4 B\nHVSP5\nD6.4-B\nDiscrete\nHVS-SASS-Pilot\nAirLab Terrace\nOutdoor\nQuartz Filter\n...\nCassie Heinle\n28/11/2022\nPhe-Chlo method (AirLab Spain)\n111\n17\nQuantus Promega (AirLab Spain)\n5\n-20C MPL Freezer (New)\nYee Hui\nAirLab Spain\n\n\n\n\n5 rows × 28 columns\n\n\n\n\nmetadata_df = (metadata_df\n .drop(columns=metadata_df.nunique().loc[lambda x: x==1].index)\n .rename(columns={'Seq ID': 'sample_id'})\n\n)\n\nThe total read count for each assigned species across all samples is loaded from /data/REL_hvsp_sp.txt\n\nspecies_df = pd.read_table('../data/REL_hvsp_sp.txt')\n\nSpecies identified as contaminants will be excluded from the analysis:\n\ncontamination_list = [\n    \"Galendromus occidentalis\", \n    \"Bemisia tabaci\", \n    \"Candidatus Portiera aleyrodidarum\", \n    \"Frankliniella occidentalis\", \n    \"Thrips palmi\"\n]\n\nInformation on the total DNA yield of each sample is stored in the /data/DNA.xlsx file:\n\ndna_yields = (pd.read_excel('../data/DNA.xlsx')\n .rename(columns={\n    'DNA Concentration (ng/uL)': 'dna_conc',\n    'DNA Volume of Elution (uL)': 'dna_vol',\n    'Sample ID': 'sample_code'\n })\n .assign(dna_yield=lambda x: x['dna_conc'] * x['dna_vol'])\n [['sample_code', 'dna_yield']]\n)\n\nWe generate a long-form dataframe with the species counts for each sample:\n\nspecies_long = (species_df\n .rename(columns={'#Datasets': 'name'})\n .melt('name', var_name='sample_id', value_name='reads')\n .assign(sample_id=lambda x: x['sample_id'].str.split('.').str[0])\n .query('name not in @contamination_list')\n .query('reads &gt; 0')\n)\nspecies_long.sample(10)\n\n\n\n\n\n\n\n\nname\nsample_id\nreads\n\n\n\n\n100599\nNocardioides islandensis\nHVSP12\n88.0\n\n\n87172\nGranulibacter bethesdensis\nHVSP11\n38.0\n\n\n152170\nKaistella daneshvariae\nHVSP19\n68.0\n\n\n164000\nArchangium violaceum\nHVSP20\n56.0\n\n\n32121\nPseudomonas cremoris\nHVSP4\n38.0\n\n\n308929\nHaloechinothrix halophila\nHVSP41\n25.0\n\n\n582584\nArthrobacter sp. SX1312\nHVSP72\n31.0\n\n\n153999\nSphingomonas sp. TZW2008\nHVSP19\n42.0\n\n\n236305\nMedicago truncatula\nHVSP30\n179.0\n\n\n125319\nNovosphingobium sp. AP12\nHVSP15\n64.0\n\n\n\n\n\n\n\n\nExperiment 1: Continuous vs Discrete sampling\nLet’s begin by examining how the DNA yield of each sample compares to the species diversity (measured as species richness):\n\n\nShow Code\nexp_1_samples = ['C3-B', 'D3.1-B', 'D3.2-B', 'D3.3-B', 'C6-B', 'D6.1-B', 'D6.2-B',\n                  'D6.3-B', 'D6.4-B', 'D6.5-B', 'D6.6-B',]\nexp1_names = [s.split('-')[0] for s in exp_1_samples]\nexp_1_meta = (metadata_df\n .query('`Sample ID` in @exp_1_samples')\n .assign(sampling_days=lambda x: x['Sample ID'].str[1])\n .rename(columns={'Sample ID': 'sample_code', 'Sampling': 'mode'})\n [['sample_id', 'sample_code', 'mode', 'sampling_days']]\n .merge(dna_yields, on='sample_code')\n)\n\n(species_long\n .query('reads &gt; 0')\n .groupby('sample_id')\n .size()\n .reset_index(name='richness')\n .merge(exp_1_meta)\n .assign(sample_code=lambda dd: dd.sample_code.str.split('-').str[0])\n .assign(sample_code=lambda dd: \n         pd.Categorical(dd.sample_code, categories=exp1_names, ordered=True))\n .melt(id_vars=['sample_id', 'sample_code', 'mode', 'sampling_days'])\n .replace({'richness': 'Species richness', 'dna_yield': 'DNA yield (ng)'})\n .pipe(lambda x: p9.ggplot(x) \n       + p9.aes(x='sample_code', y='value', fill='mode')\n       + p9.geom_col()\n       + p9.facet_wrap('~variable', scales='free_y', ncol=1)\n       + p9.annotate('vline', xintercept=[4.5], linetype='dashed', size=.4)\n       + p9.labs(x='', y='', fill='')\n       + p9.theme(\n           figure_size=(4, 4),\n           axis_text_x=p9.element_text(angle=45),\n           legend_position='top',\n           legend_key_size=10,\n           legend_text=p9.element_text(size=7),\n           )\n       )\n)\n\n\n\n\n\n\n\n\n\nThe continuous sampling method yields more DNA, which is expected given the longer sampling durations. However, this higher DNA yield does not correspond to increased species richness. On the contrary, discrete sampling, which yields lower DNA amounts, shows relatively high species richness.\nWhen we look beyond species richness to examine the specific species present in each sampling method, we see that, while most species are common to both approaches, about 25% of species are unique to discrete sampling, compared to only around 3% unique to continuous sampling suggesting that discrete sampling may capture a broader diversity of species.\n\n\nShow Code\ntable_3 = (species_long\n .merge(exp_1_meta)\n .query('sampling_days.notna()')\n .query('reads &gt; 0')\n .query('sampling_days == \"3\"')\n .groupby(['name', 'mode', 'sampling_days'])\n ['reads']\n .sum()\n .reset_index()\n .pivot(index='name', columns=['mode'], values='reads')\n .fillna(0)\n .loc[lambda x: x.sum(axis=1) &gt; 0]\n .applymap(lambda x: x &gt; 0)\n .assign(presence=lambda dd: \n         np.where(dd['Continuous'] == dd['Discrete'], 'Both',\n         np.where(dd['Continuous'], 'Only Continuous', 'Only Discrete'))\n         )\n .dropna()\n .groupby(['presence'])\n .size()\n .reset_index()\n .rename(columns={0:'richness'})\n .assign(freq=lambda dd: dd['richness'] / dd['richness'].sum())\n .assign(days=\"3 days\")\n )\n\ntable_6 = (species_long\n .merge(exp_1_meta)\n .query('sampling_days.notna()')\n .query('reads &gt; 0')\n .query('sampling_days == \"6\"')\n .groupby(['name', 'mode', 'sampling_days'])\n ['reads']\n .sum()\n .reset_index()\n .pivot(index='name', columns=['mode'], values='reads')\n .fillna(0)\n .loc[lambda x: x.sum(axis=1) &gt; 0]\n .applymap(lambda x: x &gt; 0)\n .assign(presence=lambda dd: \n         np.where(dd['Continuous'] == dd['Discrete'], 'Both',\n         np.where(dd['Continuous'], 'Only Continuous', 'Only Discrete'))\n         )\n .dropna()\n .groupby(['presence'])\n .size()\n .reset_index()\n .rename(columns={0:'richness'})\n .assign(freq=lambda dd: dd['richness'] / dd['richness'].sum())\n .assign(days=\"6 days\")\n )\n\n\n(pd.concat([table_3, table_6])\n .pipe(lambda dd: p9.ggplot(dd)\n       + p9.aes('days', 'freq')\n       + p9.geom_col(p9.aes(fill='presence'), width=.5)\n       + p9.scale_y_continuous(labels=percent_format())\n       + p9.scale_fill_manual(['#4B9C78', '#D85E01', '#7B76B5'\n           \n       ])\n       + p9.labs(x='Sampling Duration', y='Percentage of unique species', fill='')\n       + p9.theme(figure_size=(4, 3),\n                  legend_position='top'\n                  )\n\n\n )\n       )\n\n\n\n\n\n\n\n\n\nIf we represent the same information in a Sankey diagram:\n\n\nShow Code\nImage('../output/sankey_3d_vs_6d.png')\n\n\n\n\n\n\n\n\n\n\n\nExperiment 2: PM10 vs PM2.5 heads\nIf we take a look now at the 3 daily samples taken with a PM10 head and the 3 daily samples taken with a PM2.5 head, focusing on the distribution of identified species in these samples:\n\n\nShow Code\n(species_long\n.merge(metadata_df[['Sampling', 'Sample ID', 'sample_id']])\n.rename(columns={'Sample ID': 'sample_code'})\n.loc[lambda dd: dd.sample_code.str.contains('PM')]\n.loc[lambda dd: dd.sample_code.str.endswith('-B')]\n.loc[lambda dd: dd.sample_code.str.contains('.1.', regex=False)]\n.assign(header=lambda dd: np.where(dd.sample_code.str.contains(\"2.5\", regex=False),\n                                   'PM2.5', 'PM10'\n                                   ))\n.query('reads &gt; 0')\n.pivot_table(index=['name'], columns='header', values='reads', aggfunc='sum')\n.assign(presence=lambda dd: np.where(dd['PM10'].isna(), 'PM2.5',\n                            np.where(dd['PM2.5'].isna(), 'PM10', 'Both')))\n.presence.value_counts()\n)\n\n\npresence\nBoth     3634\nPM10      841\nPM2.5     740\nName: count, dtype: int64\n\n\nWe see that in this case, from a total of 5215 species, 3634 appear in both types of samples, 841 are unique to PM10 samples and 740 are unique to PM2.5 samples.\nI am going to now generate a table listing the species unique to each sample type and the count of unique samples where they appear. The formatted tables can be accessed here.\n\n\nShow Code\nuniquely_unique = (species_long\n.merge(metadata_df[['Sampling', 'Sample ID', 'sample_id']])\n.rename(columns={'Sample ID': 'sample_code'})\n.query('name not in @contamination_list')\n.loc[lambda dd: dd.sample_code.str.contains('PM')]\n.loc[lambda dd: dd.sample_code.str.endswith('-B')]\n.loc[lambda dd: dd.sample_code.str.contains('.1.', regex=False)]\n.assign(header=lambda dd: np.where(dd.sample_code.str.contains(\"2.5\", regex=False),\n                                   'PM2.5', 'PM10'\n                                   ))\n.query('reads &gt; 0')\n.groupby(['name', 'header'])\n.size()\n.rename('n_present')\n.reset_index()\n.pivot(index='name', columns='header', values='n_present')\n.fillna(0)\n.astype(int)\n.query('`PM2.5`==0 or `PM10`==0')\n.query('`PM2.5`&gt;=1 or `PM10`&gt;=1')\n.index\n.values\n)\n\nunique_pm_reads = (species_long\n.merge(metadata_df[['Sampling', 'Sample ID', 'sample_id']])\n.rename(columns={'Sample ID': 'sample_code'})\n.query('name not in @contamination_list')\n.loc[lambda dd: dd.sample_code.str.contains('PM')]\n.loc[lambda dd: dd.sample_code.str.endswith('-B')]\n.loc[lambda dd: dd.sample_code.str.contains('.1.', regex=False)]\n.assign(header=lambda dd: np.where(dd.sample_code.str.contains(\"2.5\", regex=False),\n                                   'PM2.5', 'PM10'\n                                   ))\n.query('reads &gt; 0')\n.query('name in @uniquely_unique')\n.groupby(['name', 'header'])\n.agg({'reads': sum, 'sample_code': 'nunique'})\n.reset_index()\n)\n\n(unique_pm_reads\n .pivot(index='name', columns='header')\n .fillna(0)\n .astype(int)\n).to_csv('../output/table_unique_species_pm.csv')\n\n\nUpon reviewing the total number of reads assigned per sample day and head type, we observe that the number of reads detected on Days 1 and 2 is very similar for both heads. However, on the third day, the PM2.5 head shows a significantly lower number of reads compared to the PM10 head. This is an intriguing observation, as we would expect PM10 to include the PM2.5 fraction, suggesting that PM10 should have a higher mass. While this assumption holds true in terms of DNA yield, it is not reflected in two out of the three experiments.\n\n\nShow Code\n(species_long\n.merge(metadata_df[['Sampling', 'Sample ID', 'sample_id']])\n.rename(columns={'Sample ID': 'sample_code'})\n.query('name not in @contamination_list')\n.loc[lambda dd: dd.sample_code.str.contains('PM')]\n.loc[lambda dd: dd.sample_code.str.endswith('-B')]\n.loc[lambda dd: dd.sample_code.str.contains('.1.', regex=False)]\n.assign(header=lambda dd: np.where(dd.sample_code.str.contains(\"2.5\", regex=False),\n                                   'PM2.5', 'PM10'\n                                   ))\n.query('reads &gt; 0')\n.groupby(['header', 'sample_code'])\n.reads.sum()\n.reset_index()\n.assign(sample_code=lambda dd: dd.sample_code.str.split('-').str[0])\n.assign(day=lambda dd: dd.sample_code.str.split('.').str[-1])\n.pipe(lambda dd: p9.ggplot(dd)\n      + p9.aes('day', 'reads', fill='header')\n      + p9.geom_col(position='dodge')\n      + p9.coord_flip()\n      + p9.scale_fill_manual(['#D85E01', '#7B76B5'])\n      + p9.labs(x='Discrete Sampling Day', y='Total Reads', fill='Sampling head')\n      + p9.theme(\n          figure_size=(4, 3),\n          legend_position=(.8, .85),\n          legend_key_size=10,\n          legend_title=p9.element_text(x=80, y=50, size=10)\n          )\n      )\n)\n\n\n\n\n\n\n\n\n\nBy focusing on species that are unique to each head type and present with higher relative abundance in one of the types, we can identify the top 50 most differentially abundant species. Our analysis reveals that the majority of the taxa more prevalent in the PM2.5 fraction compared to PM10 are bacterial species.\n\n\nShow Code\nspecies_by_header = (species_long\n.merge(metadata_df[['Sampling', 'Sample ID', 'sample_id']])\n.rename(columns={'Sample ID': 'sample_code'})\n.query('name not in @contamination_list')\n.loc[lambda dd: dd.sample_code.str.contains('PM')]\n.loc[lambda dd: dd.sample_code.str.endswith('-B')]\n.loc[lambda dd: dd.sample_code.str.contains('.1.', regex=False)]\n.assign(header=lambda dd: np.where(dd.sample_code.str.contains(\"2.5\", regex=False),\n                                   'PM2.5', 'PM10'\n                                   ))\n.groupby(['header', 'sample_code'], as_index=False)\n.apply(lambda dd: dd.assign(rel_abundance=lambda x: x['reads'] / x['reads'].sum()))\n.reset_index(drop=True)\n.groupby(['name', 'header'])\n.agg({'rel_abundance': lambda x: x.sum() / 3, 'reads': 'sum'})\n.loc[lambda x: x.reads &gt; 0]\n.reset_index()\n.pivot(index='name', columns='header', values=['rel_abundance', 'reads'])\n.fillna(0)\n.assign(diff=lambda dd: (dd[('rel_abundance',  'PM2.5')] - dd[('rel_abundance',  'PM10')]))\n.assign(direction=lambda dd: np.where(dd['diff'] &gt; 0, 'PM2.5 &gt; PM10', 'PM2.5 &lt; PM10'))\n.assign(abs_diff=lambda dd: dd['diff'].abs())\n.sort_values('diff', ascending=False)\n.head(50)\n.assign(reads_label=lambda dd: \n        dd['reads', 'PM10'].astype(int).astype(str) + ':' + \n        dd['reads', 'PM2.5'].astype(int).astype(str))\n.reset_index()\n.assign(direction=lambda dd: \n        pd.Categorical(dd.direction, categories=['PM2.5 &gt; PM10', 'PM2.5 &lt; PM10'],\n                       ordered=True)\n)\n .replace({\"Candidatus Symbiopectobacterium sp. 'North America'\":\n           \"Candidatus Symbiopectobacterium sp.\"})\n)\n\nsorted_species_rel = species_by_header.sort_values(('rel_abundance', 'PM2.5'))['name']\nsorted_species_reads = species_by_header.sort_values(('reads', 'PM10'))['name']\n\nspecies_by_header.columns = ['_'.join(col).strip() for col in species_by_header.columns.values]\n(species_by_header\n .assign(rel_abundance_PM10=lambda dd: dd['rel_abundance_PM10'] * - 1)\n .assign(label_pm25=lambda dd: dd['reads_PM2.5'].astype(int).astype(str) +  ' ('\n         + (dd['rel_abundance_PM2.5'] * 100).round(2).astype(str) + '%)')\n .assign(label_pm10=lambda dd: dd['reads_PM10'].astype(int).astype(str) +  ' ('\n         + (dd['rel_abundance_PM10'].abs() * 100).round(2).astype(str) + '%)')\n.assign(name=lambda dd: pd.Categorical(dd.name_, categories=sorted_species_rel, ordered=True))\n.pipe(lambda dd: p9.ggplot(dd) \n      + p9.aes('name')\n      + p9.coord_flip() \n      + p9.geom_col(p9.aes(y='rel_abundance_PM2.5'), fill='#7B76B5', width=.6)\n      + p9.geom_col(p9.aes(y='rel_abundance_PM10'), fill='#D85E01', width=.6)\n      + p9.geom_text(p9.aes(label='label_pm10', y='rel_abundance_PM10'),\n                      size=5, ha='right', nudge_y=-.0005)\n      + p9.geom_text(p9.aes(label='label_pm25', y='rel_abundance_PM2.5'),\n                      size=5, ha='left', nudge_y=.0005)\n      + p9.scale_y_continuous(\n          labels=[\"2%\", \"0%\", \"2%\"],\n          limits=[-.034, .034])\n      + p9.annotate('hline', yintercept=0, linetype='solid', size=.2)\n      + p9.labs(x='', y='Mean Relative Abundance', title=f'PM10              PM2.5')\n      + p9.theme(figure_size=(4.5, 5),\n                 axis_text_y=p9.element_text(size=6.5),\n                 axis_title_x=p9.element_text(size=9),\n                 plot_title=p9.element_text(size=10, ha='center'),\n                 )\n      )\n)\n\n\n\n\n\n\n\n\n\nIf we instead focus on the species that appear in at least 2 samples of one head type and none of the other, the top 30 for each head type are:\n\n\nShow Code\nf = (unique_pm_reads\n     .query('sample_code &gt;= 2')\n .groupby('header')\n .apply(lambda dd: dd\n        .sort_values('reads', ascending=False)\n        .reset_index(drop=True)\n        .assign(rank=lambda x: x.index + 1)\n        )\n .reset_index(drop=True)\n .query('rank &lt;= 30')\n .replace({'PM10': 'Present only in PM10', 'PM2.5': 'Present only in PM2.5'})\n .pipe(lambda dd: p9.ggplot(dd)\n       + p9.aes('reorder(name, reads)', 'reads')\n       + p9.geom_col(p9.aes(fill='header'))\n       + p9.facet_wrap('header', scales='free_y', ncol=1)\n       + p9.coord_flip()\n       + p9.labs(x='', y='')\n       + p9.scale_fill_manual(['#D85E01', '#7B76B5'])\n       + p9.guides(fill=False)\n       + p9.theme(figure_size=(5, 7))\n       \n       )\n)\n\nf.save('../output/unique_pm_species.svg', dpi=300)\nf.draw()\n\n\n\n\n\n\n\n\n\nIf we go ahead and test whether the richness of species is different between the two head types, we can do a non-parametric paired test (Wilcoxon signed-rank test) to compare the richness of species between the two head types:\n\n24 * 7\n\n168\n\n\n\n\nShow Code\npm_richness = (species_long\n.merge(metadata_df[['Sampling', 'Sample ID', 'sample_id']])\n.rename(columns={'Sample ID': 'sample_code'})\n.query('name not in @contamination_list')\n.loc[lambda dd: dd.sample_code.str.contains('PM')]\n.loc[lambda dd: dd.sample_code.str.endswith('-B')]\n# .loc[lambda dd: dd.sample_code.str.contains('.1.', regex=False)]\n.assign(header=lambda dd: np.where(dd.sample_code.str.contains(\"2.5\", regex=False),\n                                   'PM2.5', 'PM10'\n                                   ))\n.query('reads &gt; 0')\n.groupby(['header', 'sample_code'])\n['name']\n.nunique()\n.reset_index()\n.assign(day=['1 day (1)', '1 day (2)', '1 day (3)', '3 days', '6 days'] * 2)\n.pivot(index='day', columns='header', values='name')\n.assign(diff=lambda dd: (dd['PM2.5'] - dd['PM10']))\n)\n\npm_richness\n\n\n\n\n\n\n\n\nheader\nPM10\nPM2.5\ndiff\n\n\nday\n\n\n\n\n\n\n\n1 day (1)\n3682\n3626\n-56\n\n\n1 day (2)\n3019\n3232\n213\n\n\n1 day (3)\n1599\n642\n-957\n\n\n3 days\n3725\n3650\n-75\n\n\n6 days\n1016\n503\n-513\n\n\n\n\n\n\n\nThe results of the test show a non-significant difference in the richness of species between the two head types (p-value = 0.75).\n\n(species_long\n.merge(metadata_df[['Sampling', 'Sample ID', 'sample_id']])\n.rename(columns={'Sample ID': 'sample_code'})\n.loc[lambda dd: dd.sample_code.str.contains('PM')]\n.loc[lambda dd: dd.sample_code.str.endswith('-B')]\n.assign(header=lambda dd: np.where(dd.sample_code.str.contains(\"2.5\", regex=False),\n                                   'PM2.5', 'PM10'\n                                   ))\n[['sample_id', 'sample_code', 'header']]\n.drop_duplicates()\n.merge(dna_yields)\n.assign(sample_comp=['3 days', '3 days', '6 days', '6 days', '1 day (1)'\n                     , '1 day (1)', '1 day (2)', '1 day (2)', '1 day (3)', '1 day (3)'])\n)\n\n\n\n\n\n\n\n\nsample_id\nsample_code\nheader\ndna_yield\nsample_comp\n\n\n\n\n0\nHVSP22\nPM10.3-B\nPM10\n60.746667\n3 days\n\n\n1\nHVSP23\nPM2.5.3-B\nPM2.5\n8.426333\n3 days\n\n\n2\nHVSP49\nPM10.6-B\nPM10\n38.590000\n6 days\n\n\n3\nHVSP50\nPM2.5.6-B\nPM2.5\n10.795000\n6 days\n\n\n4\nHVSP51\nPM10.1.1-B\nPM10\n13.345000\n1 day (1)\n\n\n5\nHVSP52\nPM2.5.1.1-B\nPM2.5\n2.567000\n1 day (1)\n\n\n6\nHVSP53\nPM10.1.2-B\nPM10\n31.336667\n1 day (2)\n\n\n7\nHVSP54\nPM2.5.1.2-B\nPM2.5\n5.264333\n1 day (2)\n\n\n8\nHVSP55\nPM10.1.3-B\nPM10\n35.643333\n1 day (3)\n\n\n9\nHVSP56\nPM2.5.1.3-B\nPM2.5\n26.123333\n1 day (3)\n\n\n\n\n\n\n\n\n(species_long\n.merge(metadata_df[['Sampling', 'Sample ID', 'sample_id']])\n.rename(columns={'Sample ID': 'sample_code'})\n.loc[lambda dd: dd.sample_code.str.contains('PM')]\n.loc[lambda dd: dd.sample_code.str.endswith('-B')]\n.assign(header=lambda dd: np.where(dd.sample_code.str.contains(\"2.5\", regex=False),\n                                   'PM2.5', 'PM10'\n                                   ))\n[['sample_id', 'sample_code', 'header']]\n.drop_duplicates()\n.merge(dna_yields)\n.assign(sample_comp=['3 days', '3 days', '6 days', '6 days', '1 day (1)'\n                     , '1 day (1)', '1 day (2)', '1 day (2)', '1 day (3)', '1 day (3)'])\n.pipe(lambda dd: p9.ggplot(dd)\n      + p9.aes('sample_comp', 'dna_yield', fill='header')\n      + p9.geom_col(position='dodge')\n      + p9.coord_flip()\n      + p9.scale_fill_manual(['#D85E01', '#7B76B5'])\n      + p9.labs(x='', y='DNA Yield (ng)', fill='') \n      + p9.theme(figure_size=(4, 3),\n                 legend_position=(.95, .05),)\n)\n    \n    #   )\n\n)\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import shapiro, ttest_rel, wilcoxon, ttest_ind, mannwhitneyu\n\n# Updated dataset\nextended_data = pd.DataFrame({\n    'sample_id': ['HVSP22', 'HVSP23', 'HVSP49', 'HVSP50', 'HVSP51',\n                  'HVSP52', 'HVSP53', 'HVSP54', 'HVSP55', 'HVSP56'],\n    'sample_code': ['PM10.3-B', 'PM2.5.3-B', 'PM10.6-B', 'PM2.5.6-B', 'PM10.1.1-B',\n                    'PM2.5.1.1-B', 'PM10.1.2-B', 'PM2.5.1.2-B', 'PM10.1.3-B', 'PM2.5.1.3-B'],\n    'header': ['PM10', 'PM2.5', 'PM10', 'PM2.5', 'PM10',\n               'PM2.5', 'PM10', 'PM2.5', 'PM10', 'PM2.5'],\n    'dna_yield': [60.75, 8.42, 38.59, 10.79, 13.34,\n                  2.57, 31.33, 5.26, 35.64, 26.12],\n})\n\n# Extract PM10 and PM2.5 data\npm10_yields = extended_data[extended_data['header'] == 'PM10']['dna_yield'].values\npm25_yields = extended_data[extended_data['header'] == 'PM2.5']['dna_yield'].values\n\n# Normality test on paired differences\ndifferences_extended = pm10_yields - pm25_yields\nshapiro_test_extended = shapiro(differences_extended)\n\n# Paired t-test\npaired_t_test_extended = ttest_rel(pm10_yields, pm25_yields)\n\n# Wilcoxon signed-rank test\nwilcoxon_test_extended = wilcoxon(pm10_yields, pm25_yields)\n\n# Unpaired t-test (Welch’s test)\nunpaired_t_test_extended = ttest_ind(pm10_yields, pm25_yields, equal_var=False)\n\n# Mann-Whitney U test\nmannwhitney_test_extended = mannwhitneyu(pm10_yields, pm25_yields, alternative='two-sided')\n\n# Print results\nprint(\"Shapiro-Wilk Normality Test:\", shapiro_test_extended)\nprint(\"Paired t-test:\", paired_t_test_extended)\nprint(\"Wilcoxon signed-rank test:\", wilcoxon_test_extended)\nprint(\"Unpaired t-test:\", unpaired_t_test_extended)\nprint(\"Mann-Whitney U test:\", mannwhitney_test_extended)\n\nShapiro-Wilk Normality Test: ShapiroResult(statistic=0.8854308128356934, pvalue=0.33464905619621277)\nPaired t-test: TtestResult(statistic=3.269238066491219, pvalue=0.03081127353194371, df=4)\nWilcoxon signed-rank test: WilcoxonResult(statistic=0.0, pvalue=0.0625)\nUnpaired t-test: Ttest_indResult(statistic=2.9276970127016364, pvalue=0.02556940516470238)\nMann-Whitney U test: MannwhitneyuResult(statistic=24.0, pvalue=0.015873015873015872)\n\n\n\n(species_long\n.merge(metadata_df[['Sampling', 'Sample ID', 'sample_id']])\n.rename(columns={'Sample ID': 'sample_code'})\n.loc[lambda dd: dd.sample_code.str.contains('PM')]\n.loc[lambda dd: dd.sample_code.str.endswith('-B')]\n.loc[lambda dd: dd.sample_code.str.contains('.1.', regex=False)]\n.assign(header=lambda dd: np.where(dd.sample_code.str.contains(\"2.5\", regex=False),\n                                   'PM2.5', 'PM10'\n                                   ))\n[['sample_id', 'sample_code', 'header']]\n.drop_duplicates()\n.merge(dna_yields)\n.assign(sample_day=lambda dd: dd.sample_code.str[-5:-2])\n\n)\n\n\n\n\n\n\n\n\nsample_id\nsample_code\nheader\ndna_yield\nsample_day\n\n\n\n\n0\nHVSP51\nPM10.1.1-B\nPM10\n13.345000\n1.1\n\n\n1\nHVSP52\nPM2.5.1.1-B\nPM2.5\n2.567000\n1.1\n\n\n2\nHVSP53\nPM10.1.2-B\nPM10\n31.336667\n1.2\n\n\n3\nHVSP54\nPM2.5.1.2-B\nPM2.5\n5.264333\n1.2\n\n\n4\nHVSP55\nPM10.1.3-B\nPM10\n35.643333\n1.3\n\n\n5\nHVSP56\nPM2.5.1.3-B\nPM2.5\n26.123333\n1.3\n\n\n\n\n\n\n\n\n(metadata_df\n [['Sampling', 'Sample ID', 'sample_id']]\n .assign(header=lambda dd: np.where(dd.sample_id.str.contains(\"2.5\", regex=False),'PM2.5', 'PM10')\n )\n )\n\n\n\n\n\n\n\n\nSampling\nSample ID\nsample_id\nheader\n\n\n\n\n\n\n\n\n\n\nstats.wilcoxon(pm_richness['diff'])\n\nWilcoxonResult(statistic=2.0, pvalue=0.75)\n\n\n\n\nSupp. Experiment: Bags vs Vial blanks\nWe are going to compare the richness of the blanks using vials and bags to see if there is a significant difference between the two types of blanks:\n\nbag_vial_blanks = ['B6-B', 'B6-V', 'B3-B', 'BPM-B', 'B3-V', 'BPM-V', 'BQZ-B', 'BQZ-V']\n\n\nblanks_table = pd.read_csv('../data/ABS_hvs_blank2-sp.csv')\nblanks_table.columns = [col.split('.')[0] for col in blanks_table.columns]\nblanks_table.rename(columns={'#Datasets': 'name'}, inplace=True)\n\n\n\nShow Code\nblanks_richness =(blanks_table\n .melt('name', var_name='sample_id', value_name='reads')\n .merge(metadata_df[['Sample ID', 'sample_id']])\n .query('`Sample ID` in @bag_vial_blanks')\n .assign(kind=lambda dd: dd['Sample ID'].str.split('-').str[1]\n         .replace({'B': 'Bag', 'V': 'Vial'}))\n .assign(sample_name=lambda dd: dd['Sample ID'].str.split('-').str[0])\n .query('reads &gt; 0')\n .groupby(['sample_name', 'kind'])\n .agg({'name': 'nunique'})\n .reset_index()\n)\n\n(blanks_richness\n .pipe(lambda dd: p9.ggplot(dd)\n        + p9.aes('sample_name', 'name', fill='kind')\n        + p9.geom_col(position='dodge', width=.7)\n        + p9.coord_flip()\n        + p9.labs(x='', y='Species Richness', fill='')\n        + p9.scale_fill_manual(['#D85E01', '#7B76B5'])\n        + p9.theme(figure_size=(4, 3),\n                   legend_position='top',\n                   legend_key_size=11,\n                   )\n       )\n)\n\n\n\n\n\n\n\n\n\nThere is no significant difference when testing with a Wilcoxon signed-rank test (p-value = 0.375):\n\nblanks_richness_diffs = (blanks_richness\n .pivot(index='sample_name', columns='kind', values='name')\n .eval('diff = Vial - Bag')\n)\n\nblanks_richness_diffs.pipe(lambda dd: stats.wilcoxon(dd['diff']))\n\nWilcoxonResult(statistic=2.0, pvalue=0.375)\n\n\nWe can represent this comparison also with a boxplot:\n\n\nShow Code\nblanks_richness.pipe(\n    lambda dd: p9.ggplot(dd)\n    + p9.aes(x='kind', y='name', color='kind')\n    + p9.geom_boxplot()\n    + p9.geom_point()                 \n    + p9.coord_flip()\n    + p9.scale_color_manual(['#D85E01', '#7B76B5'])\n    + p9.theme(figure_size=(4, 3))\n    + p9.guides(color=False)\n    + p9.labs(x='', y='Species Richness')\n)\n\n\n\n\n\n\n\n\n\nOr with a lineplot showing the paired differences:\n\n\nShow Code\nblanks_richness.pipe(\n    lambda dd: p9.ggplot(dd)\n    + p9.aes(x='kind', y='name', color='kind')\n    + p9.geom_line(p9.aes(group='sample_name', y='name'), color='gray', linetype='dotted')\n    + p9.geom_point()\n    + p9.scale_color_manual(['#D85E01', '#7B76B5'])\n    + p9.geom_text(p9.aes(label='sample_name'), nudge_x=.075, size=7,\n                    data=dd.query('kind == \"Vial\"'), color='black')\n    + p9.theme(figure_size=(4, 3))\n    + p9.guides(color=False)             \n    + p9.scale_x_discrete(expand=(.1, .1))      \n    + p9.labs(x='', y='Species Richness')\n)\n\n\n\n\n\n\n\n\n\nA classical barplot would look like this (but it’s not very informative since it obscures the paired nature of the data):\n\n\nShow Code\n(blanks_richness.pipe(\n    lambda dd: p9.ggplot(dd)\n    + p9.aes(x='kind', y='name', fill='kind')\n    + p9.stat_summary(geom='errorbar', \n                    fun_ymax=lambda x: np.mean(x) + np.std(x),\n                    fun_ymin=lambda x: np.mean(x) - np.std(x),\n                    width=.5)\n    + p9.scale_fill_manual(['#D85E01', '#7B76B5'])\n    + p9.guides(fill=False)\n    + p9.stat_summary(geom='col', fun_y=np.mean)\n    + p9.theme(figure_size=(4, 3))\n    + p9.labs(x='', y='Species Richness')\n)\n)\n\n\n\n\n\n\n\n\n\n\n\nShannon and Simpson diversity indexes\nWe are now going to compute the Shannon and Simpson diversity indexes for the blanks.\nThe Shannon diversity index H is given by the formula:\n\\[H = -\\sum_{i=1}^{R} p_i \\ln(p_i)\\]\nwhere:\n\n\\(R\\) is the total number of species,\n\\(p_i\\) is the proportion of individuals belonging to the \\(i\\)-th species, calculated as \\(p_i = \\frac{n_i}{N}\\) ,\n\\(n_i\\) is the number of individuals in the i -th species,\n\\(N\\) is the total number of individuals across all species.\n\nThe Simpson diversity index D can then be defined as:\n\\[D = 1 - \\sum_{i=1}^{R} p_i^2  = 1 - \\frac{\\sum_{i=1}^{R} n_i (n_i - 1)}{N (N - 1)} \\]\nWe’ll define a function to calculate each of these indexes in numpy and then simply apply it to the columns of the dataframe with the species counts to get the diversity indexes for each sample:\n\nfrom typing import List\n\ndef shannon_diversity(species_counts: List[int]) -&gt; float:\n    species_counts = np.array(species_counts)\n    species_counts = species_counts[species_counts &gt; 0]\n    total_counts = np.sum(species_counts)\n    proportions = species_counts / total_counts\n    shannon_index = -np.sum(proportions * np.log(proportions))\n    return shannon_index\n\ndef simpson_diversity(species_counts: List[int]) -&gt; float:\n    species_counts = np.array(species_counts)\n    total_counts = np.sum(species_counts)\n    proportions = species_counts / total_counts\n    simpson_index = 1 - np.sum(proportions ** 2)\n    return simpson_index\n\n\n\nShow Code\nblank_sample_names = metadata_df.loc[metadata_df['Sample ID'].str.startswith('B')]['Sample ID']\n\n(blanks_table\n .melt('name', var_name='sample_id', value_name='counts')\n .merge(metadata_df[['Sample ID', 'sample_id']])\n .query('`Sample ID` in @blank_sample_names')\n .pivot(index='name', columns='Sample ID', values='counts')\n .fillna(0)\n .pipe(lambda dd: pd.concat([\n        dd.apply(shannon_diversity, axis=0).rename('Shannon Index').reset_index()\n        .merge(dd.apply(simpson_diversity, axis=0).rename('Simpson Index').reset_index())\n    ])\n )\n .melt('Sample ID', var_name='index', value_name='value')\n .query('`Sample ID` != \"BHVS-SG\"')\n .pipe(lambda dd: p9.ggplot(dd)\n       + p9.aes('Sample ID', 'value')\n       + p9.geom_col()\n       + p9.facet_wrap('~index', scales='free_y', ncol=1)\n       + p9.labs(x='', y='')\n       + p9.theme(figure_size=(4, 3), axis_text_x=p9.element_text(angle=45))\n )\n)"
  }
]